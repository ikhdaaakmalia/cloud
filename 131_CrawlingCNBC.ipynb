{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpQx9v+QDm9uNOccJaNaf9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikhdaaakmalia/cloud/blob/main/131_CrawlingCNBC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7OFgtRJYVUd",
        "outputId": "e6c670c9-a101-4237-82a9-0097fc58cd2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymongo[srv]\n",
            "  Downloading pymongo-4.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "\u001b[33mWARNING: pymongo 4.12.1 does not provide the extra 'srv'\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting dnspython<3.0.0,>=1.16.0 (from pymongo[srv])\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymongo-4.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
            "Successfully installed dnspython-2.7.0 pymongo-4.12.1\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pymongo[srv]\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from pymongo import MongoClient\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Koneksi ke MongoDB\n",
        "client = MongoClient(\"mongodb+srv://ikhdaamel:sejutasayang@cluster0.lk8xt.mongodb.net/\")\n",
        "\n",
        "def crawl_cnbc_environment():\n",
        "    # Keyword pencarian topik sustainability = kelestarian lingkungan\n",
        "    base_url = 'https://www.cnbcindonesia.com/search?query=environmental+sustainability'\n",
        "    collection = client['Crawling_Keyword']['environment_cnbc']\n",
        "\n",
        "    # Jumlah halaman yang ingin di-crawl\n",
        "    max_page = 15\n",
        "    page = 1\n",
        "\n",
        "    while page <= max_page:\n",
        "        url = f\"{base_url}&page={page}\"\n",
        "        print(f'ğŸŒ± Crawling halaman {page}: {url}')\n",
        "        res = requests.get(url)\n",
        "        soup = BeautifulSoup(res.text, 'lxml')\n",
        "\n",
        "        articles = soup.find_all('article')\n",
        "        if not articles:\n",
        "            print('âœ… Tidak ada artikel ditemukan.')\n",
        "            break\n",
        "\n",
        "        for artikel in articles:\n",
        "            try:\n",
        "                a_tag = artikel.find('a')\n",
        "                if not a_tag or not a_tag.get('href'):\n",
        "                    continue\n",
        "                link = a_tag['href']\n",
        "                judul = a_tag.get_text(strip=True)\n",
        "\n",
        "                if collection.find_one({'link': link}):\n",
        "                    print(f'â© Skip (sudah ada): {judul}')\n",
        "                    continue\n",
        "\n",
        "                # Buka halaman berita\n",
        "                isi_res = requests.get(link)\n",
        "                isi_soup = BeautifulSoup(isi_res.text, 'lxml')\n",
        "\n",
        "                # Ambil tanggal\n",
        "                tanggal_tag = isi_soup.find('div', class_='date')\n",
        "                tanggal = tanggal_tag.get_text(strip=True) if tanggal_tag else 'Tanggal tidak ditemukan'\n",
        "\n",
        "                # Ambil author\n",
        "                author_tag = isi_soup.find('div', class_='author')\n",
        "                author = author_tag.get_text(strip=True) if author_tag else 'Author tidak ditemukan'\n",
        "\n",
        "                # Ambil tag kategori\n",
        "                kategori_tag = isi_soup.find('div', class_='breadcrumb')  # kategori bisa dari breadcrumb\n",
        "                tags = kategori_tag.get_text(\" > \", strip=True) if kategori_tag else 'Kategori tidak ditemukan'\n",
        "\n",
        "                # Ambil isi berita\n",
        "                body_content = isi_soup.find('div', class_='detail_text')\n",
        "                isi_paragraf = [p.get_text(strip=True) for p in body_content.find_all('p')] if body_content else []\n",
        "                isi_berita = ' '.join(isi_paragraf) if isi_paragraf else 'Isi berita tidak ditemukan'\n",
        "\n",
        "                # Ambil thumbnail\n",
        "                og_image = isi_soup.find('meta', property='og:image')\n",
        "                thumbnail = og_image['content'] if og_image else 'Thumbnail tidak ditemukan'\n",
        "\n",
        "                # Simpan ke database\n",
        "                collection.insert_one({\n",
        "                    'judul': judul,\n",
        "                    'link': link,\n",
        "                    'tanggal': tanggal,\n",
        "                    'author': author,\n",
        "                    'tags': tags,\n",
        "                    'isi_berita': isi_berita,\n",
        "                    'thumbnail': thumbnail,\n",
        "                    'waktu_crawl': datetime.now()\n",
        "                })\n",
        "                print(f'âœ… Tersimpan: {judul[:60]}')\n",
        "\n",
        "                time.sleep(random.uniform(0.2, 0.6))  # jeda antara crawling tiap berita\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f'âŒ Error saat memproses artikel: {e}')\n",
        "\n",
        "        page += 1  # lanjut ke halaman berikutnya\n",
        "\n",
        "# Jalankan fungsi crawler\n",
        "crawl_cnbc_environment()\n"
      ],
      "metadata": {
        "id": "aYdg9Y8hcVRX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}